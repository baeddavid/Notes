#+TITLE: Algorithms
#+DESCRIPTION: Notes on algorithms and my pseudocodes for exercises
* Algo Notes
** Trees
*** Binary Tree
Formally, a binary tree is either empty, or a root node r together with a left binary tree and a right binary tree. The subtrees themselves are binary trees. The left binary tree is sometimes referred to as the left subtree of the root, and the right binary tree is referred to as the right subtree of the root.

#+BEGIN_SRC java
public class BinaryTreeNode <T> extends TreeLike<T, BinaryTreeNode<T>> {
    publc T data;
    public BinaryTreeNode <T> left, right;
}
#+END_SRC

Each node except the root is itself the root of a left subtree or a right subtree. If l is the the root of p's left subtree, we will say l is the left child of p, and p is the parent of l; the notion of right child is similar.
*** BST
**** Traversals
***** Three main ways to traverse a tree
****** Inorder Traversal
#+BEGIN_SRC java
void inorder(Node root) {
    if(root != null) {
        inorder(root.left);
        System.out.print(root.data);
        inorder(roof.right);
    }
}
#+END_SRC
We can traverse the tree iteratively if we use a stack
#+BEGIN_SRC java
void inorder(Node root) {
    Stack <Node> s = new Stack <>();
    s.push(root);
    Node curr = root;

    while(!s.isEmpty() || curr != null) {
        while(curr != null) {
            s.push(curr);
            curr = curr.left;
        }
        curr = s.pop();
        System.out.print(curr.data);
        curr = curr.right;
    }
}
#+END_SRC
****** Preorder Traversal
#+BEGIN_SRC java
void preorder(Node root) {
    if(root != null) {
        System.out.print(root.data);
        preorder(root.left);
        preorder(root.right);
    }
}
#+END_SRC
****** Postorder Traversal
#+BEGIN_SRC java
void postorder(Node root) {
    if(root != null) {
        postorder(root.left);
        postorder(root.right);
        System.out.print(root.data);
    }
}
#+END_SRC
**** Search
Due to the nature of binary search trees, searching for an element is a fast operation taking only O(n log n) time.
This too can be done either recursively or iteratively.
***** Recursive
#+BEGIN_SRC java
Node search(Node root, int key) {
    if(root == null || key = root.data) {
        return root;
    }

    if(key < root.data) {
        return search(root.left, key);
    } else {
        return search(root.right, key);
    }
}
#+END_SRC
***** Iterative
#+BEGIN_SRC java
Node search(Node root, int key) {
    while(root != null && root.data != key) {
        if(root.data < key) {
            root = root.right;
        } else {
            root = root.left;
        }
    }
    return root;
}
#+END_SRC
**** Min and Max
Due to the sorted nature of binary search trees we can find minimum and maximum values fairly easily.
***** Minimum
#+BEGIN_SRC java
Node getMinimum(Node root) {
    while(root.left != null) {
        root = root.left;
    }
    return root;
}
#+END_SRC
***** Maximum
#+BEGIN_SRC java
Node getMaximum(Node root) {
    while(root.right != null) {
        root = root.right;
    }
    return root;
}
#+END_SRC
Although the Min and Max operations are fairly straightforward implementations they are useful in finding predecessors or sucessors for nodes.
**** Successor and Predecessors
***** Successor
If all keys in a tree are distinct, the successor of a node x is the node with the smallest key greater than x. The structure of a binary search tree allows us to determine the succssor of a node without even comparing the keys.

There are two ways to find the successor for a node depending on memory.
****** If node has parent attribute
If the node has a parent attribute the implementation is straightforward in that we need to perform two checks. If the node has a right child, we call getMinimum on the right child and return the result.

If the node does not have a right child, we walk up the tree until we reach an ancestor that has the node we called on as a left subtree.
#+BEGIN_SRC java
Node getSuccessor(Node x) {
    if(x.right != null) {
        return getMinimum(x.right);
    } else {
        Node y = x.parent;
        while(y != null && x == y.right) {
            x = y;
            y = y.parent;
        }
        return y;
    }
}
#+END_SRC
****** If the node does not have a parent attribute
If the node does not have a parent attribute we need to be a little bit more clever. What we do instead is pass three parameters into a function. One the root of the tree, one the target, and the last a null value that we will use to store candidates for successors.

If the data at root is equal to the key of the node we want to find the successor and it has a right child we call getMinimum and return its value.

If the data at root is less than the key we recurse leftwards such that root <-- root.left and successor <-- root.

If the data at root is greater than the key we recurse righwards such that root <-- root.right. However we do not reassign successor due to the assignment violating the principle that a successor node is larger than the node we have called it on.

#+BEGIN_SRC java
Node getSuccessor(Node root, Node succ, int key) {
    if(root == null) {
        return null;
    }

    if(root.data == key) {
        if(root.right != null) {
            return getMinimum(root.right);
        }
    } else if(key < root.data) {
        succ = root;
        return getSuccessor(root.left, succ, key);
    } else {
        return getSuccessor(root.right, succ, key);
    }

    return succ;
}
#+END_SRC
**** Insertion and Deletion
***** Insertion
To insert a new value v into a binary search tree T, we use the procedure TREE-INSERT. The procedure takes a node z for which z.key = v, z.left = NIL, and z.right = NIL. It modifies T and some of the attributes of z in such a way that it inserts z into an appropriate position in the tree.

#+BEGIN_SRC java
void TreeInsert(Node root, Node insert) {
    Node y = null;
    Node x = root;

    while(x != null) {
        y = x;
        if(insert.key < x.key) {
            x = x.left;
        } else {
            x = x.right;
        }
    }
    insert.parent = y;
    if(y == null) {
        root = insert;
    } else if(insert.key < y.key) {
        y.left = current;
    } else {
        y.right = insert;
    }
}
#+END_SRC
***** Deletion
The overall strategy for deleting a node z from a binary search tree T has three basic cases.
****** If z has no children
If we have a parent attribute in the node class, we simply modify z.parent such that it no longer points to z. If z does not have a parent attribute then we iterate through the tree with two pointers, one acting as a parent pointer the other acting as the key pointer. When we find the node to be deleted we simply set the parent pointer such that it no longer points to z.
****** If z has one child
Begin with finding the child node z. Since z only has one child we need to simply remove it and append z.child to the parent pointer.
****** If z has two children
If z has two children it is a little more complicated. We begin with finding the in-order successor. Storing the inorder successor, we then recursively delete it from the tree, replacing the current pointer with the value of the successor.
******* Why we use in-order successor
We do not need to necessarily use the in-order succesor or the in-order predecessor as either one will suffice. What is important is that we find a node that is suitable to preserve the order of the binary search tree.

The only two nodes that can replace a node with two children is either the predecessor or successor.
****** Implmenetation
#+BEGIN_SRC java
Node deleteNode(Node root, int key) {
    Node parent = null;
    Node curr = root;

    // Search for the key and set the parent pointer
    while(curr != null && curr.data != key) {
        parent = curr;
        if(key < curr.data) {
            curr = curr.left;
        } else {
            curr = curr.right;
        }
    }

    // If the key is not in the tree just return.
    if(curr == null) {
        return root;
    }

    // If node to be deleted has no children
    if(curr.left == null && curr.right == null) {
        if(curr != root ){
            if(parent.left == curr) {
                parent.left = null;
            } else {
                parent.right = null;
            }
        } else {
            root = null;
        }
    }

    // If the node to be deleted has two children
    else if(curr.left != null && curr.right != null) {
        Node successor = getSuccessor(root, null, curr.data);
        int val = successor.data;
        delete(root, successor.data);
        curr.data = val;
    }

    // If the node to be deleted has one child
    else {
        // Check to see which child it is
        Node child = (curr.left != null) ? curr.left : curr.right;
        if(curr != root) {
            if(curr == parent.left) {
                parent.left = child;
            } else {
                parent.right = child;
            }
        } else {
            root = child;
        }
    }
    return root;
}
#+END_SRC

**** Thoughts on Trees
Take note that of the two operations we have already looked at we are able to use both iterative and recursive implementations. This is due to the recursive nature of the data structure.
**** Exercises
***** 12.2.1 was skipped due to needing the answer to be drawn
***** 12.2.2 Write recursive versions of TREE-MINIMUM and TREE-MAXIMUM
#+BEGIN_SRC java
Node treeMinimum(Node root) {
    if(root.left == null) {
        return root;
    }
    return treeMinimum(root.left);
}

Node treeMaximum(Node root) {
    if(root.right == null) {
        return root;
    }
    return treeMaximum(root.right);
}
#+END_SRC
***** 12.2.3 Write the TREE-PREDECESSOR procedure
Assuming that we have a parent attribute in the node
#+BEGIN_SRC java
Node treePredecessor(Node x) {
    if(x.left != null) {
        return getMaximum(x.left);
    } else {
        Node y = x.parent;
        while(y != null && x == y.left) {
            x = y;
            y = y.parent;
        }
        return y;
    }
}
#+END_SRC

If we don't have access to a parent attribute we do the same as sucessor implementation
#+BEGIN_SRC java
Node treePredecessor(Node root, Node pred, int key) {
    if(root == null) {
        return null;
    }

    if(root.data == key) {
        if(root.left != null) {
            return getMaximum(root.left);
        }
    } else if(key < root.data) {
        return treePredecessor(root.left, pred, key);
    } else {
        pred = root;
        return treePredecessor(root.right, pred, key);
    }
    return pred;
}
#+END_SRC
***** 12.2.5 Show that if a node in a binary search tree has two children then its successor has no left child and it predecessor has no right child.
Assume the we are operating on a node z which has two children. If we are to find the successor i.e. the smallest node that is larger than z, we must traverse down the left subtree for z.right. The successor is the last node in this left subtree. It cannot have a left child, as that would imply that the node we are currently on is not in fact the successor as there is another node that is smaller, but greater than z.

The same logic applies to the predecessor for z. The predecessor is the largest node smaller than it. We can acheive this by traversing the right subtree of z.left. If the "predecessor" has a right child, than we node that the current node we are on is not the predecessor as there is another node that is larger it that is smaller than z.
***** 12.3.1 Give a recursive version of the TREE-INSERT procedure.
The iterative tree insert procedure is simply a modified implementation of the TREE-SEARCH algorithm. Looking at the iterative tree insert implementation, the first step we do is search for the appropriate position of the node.

 #+BEGIN_SRC java
void insert(int data) {
    // Assuming root is a class variable
    TreeNode current = root;
    TreeNode parent = null;
    while(current != null) {
        if(data < current.data){
            parent = current;
            current = current.left;
        } else {
            parent = current;
            current = current.right;
        }
    }
    if(parent.data > data) {
        parent.left = new TreeNode(data);
    } else {
        parent.right = new TreeNode(data);
    }
}
 #+END_SRC


 A recursive implementation of the insert method would change the while loop into a recursive search.
 #+BEGIN_SRC java
void insert(TreeNode root, TreeNode parent, int data) {
    // If the root is null we can insert data into tree.
    if(root == null) {
        if(parent.data > data) {
            parent.left = new TreeNode(data);
        } else {
            parent.right = new TreeNode(data);
        }
    }

    // If the root is not null we set up our recursion
    if(data < root.data) {
        return insert(root.left, root, data);
    } else {
        return insert(root.right, root, data);
    }
}
 #+END_SRC
***** 12.3.2 Suppose that we construct a binary search tree by repeatedly inserting distinct values into the tree. Argue that the number of nodes examined in searching for a value in the tree is one plus the number of nodes examined when the value was first inserted into the tree
As mentioned in the previous answer, the TREE-INSERTION method is a modified implementation of TREE-SEARCH. Because binary search trees are sorted, inserting an element requires us to search for the correct position to insert the new node.

For example given a tree [6,4,12] and we wish to insert the value 5 we must first search for the correct position. Inserting it into tree gives us [6,4,12,null,5] with two comparisons --> 5 < 12 and 5 > 4.

Searching for the value 5 in the tree now requires three comparisons, 5 < 12, 5 > 4, 5 = 5. Why is the number of nodes examined in search for a value one plus the nuymber of nodes examined when it was first inserted?

This is because of the sorted anture of the tree. When we insert a node z into the tree, say we perform x comparisons to find the correct position. Searching for z in the tree immediately after it was inserted into the tree is now x + 1 comparison.
***** 12.3.3 We can sort a given set of n numbers by first building a binary search tree containing these numbers and printing the numbers by an inorder tree walk. What are the worst case and best case running times for this sorting algorithm?
The worst case running time for sorting an array using a binary search tree is O(n^2). This occurs when the input array is sorther either ascending or descending. This is because we must traverse the entire tree to append any element from an array A.

The best case is when the tree is balanced. Insertion into a balanced tree is a O(log n) operation since we only need to traverse a single subtree at each comparison. This means that given a perfectly balanced input, sorting the array would take O(n log n).
** Graphs
*** Undirected Graphs
A graph is a set of vertices and a collection of edges that each connect a pair of vertices.

A self-loop is an edge that connects a vertex to itself.
Two edges that connect the same pair of vertices are parallel.

There are two ways we can represent the edges between vertices.
    1. The first way is to create adjacency-list representations. Essentially every vertex in the graph has a list that contains the vertices that it is connected to.

       It is better to use adjacency-lists for sparsely populated graphs. Sparese graphs refer to the number of edges the graph has, not the number of vertices.

       1 --> {4, 6, 2, 3} means that the vertex one has an edge to vertex 4, 6, 2, and 3.
    2. The second way is adjacency-matrix representation. It is a matrix representation of the graph such that if (i, j), which are vertices, have a value of 1, then there is an edge connecting the two. 0 If there is no edge.

       It is better to use adjacency-matrices for densely populated graphs.
**** DFS (Depth First Search)
**** BFS (Breadth First Search)
Breadth first search is a way to traverse a graph. Similar to traversing a tree, traversing graphs pose a slight difficulty. Graphs may contain cycles, something that is not accounted for in traversing a tree. To avoid processing a node more than once, we use a boolean visited array.

#+BEGIN_SRC java
void BFS(int s) {
    boolean[] visited = new boolean[V];
    LinkedList <Integer> queue = new LinkedList <Integer>();
    visited[s] = true;
    queue.add(s);

    while(queue.size() != 0) {
        s = queue.poll();
        // ACTION ON s
        Iterator <Integer> i = adj[s].listIterator();
        while(i.hasNext()) {
            int n = i.next();
            if(!visited[n]) {
                visited[n] = true;
                queue.add(n);
            }
        }
    }
}
#+END_SRC

** Interview Problems
*** Maximum Depth of Binary Tree
 Given a binary tree, find its maximum depth. The maximum depth is the number of nodes along the longest path from the root node dwon to the farthest leaf node.

 Example: Given binary tree [3,9,20,null,null,15,7] return 3.

 My base instinct is to create a variable called max that will store the height of a tree. I then perform an inorder traversal of the tree keeping track of the maximum depth seen so far. Although this solution would technically work, the implementation is long and potentially messy.

 Instead there is a much more straight forward solution to this problem. Keeping in mind the recursive property of trees, we can call the function on both the left and right subtrees and then add one to the max of the two values.

 #+BEGIN_SRC java
int maxDepth(TreeNode root) {
    // If the root is null return 0.
    if(root == null) {
        return 0;
    }

    // Height of the left subtree
    int L = maxDepth(root.left);
    // Height of the right subtree
    int R = maxDepth(root.right);

    // Return the max of the two subtrees + 1 to account for the root for the two subtrees.
    return Math.max(L, R) + 1;
}
 #+END_SRC
*** Inorder Traversal
Given a binary search tree perform an inorder traversal of the tree.
Extra: Do it without recursion.

The obvious way to do it with recursion is to check if the current treeNode is null or not.
#+BEGIN_SRC java
void inOrder(TreeNode root) {
    if(root != null) {
        inOrder(root.left);
        System.out.print(root.data);
        inOrder(root.right);
    }
}
#+END_SRC

Performing an iterative traversal is a little biut more complicated. When looking at how the inorder traversal walks through a tree you will note that after going down the left most sub tree and printing the left most leaf, we then walk back up the tree. When we walk back up the tree, we check if the treeNode has a right child; if it does we begin traversing this subtree as well. What you will notice is that this is somewhat similar to how a stack works, we use the last node we have, then go back up the tree or "pop" it off the stack.

#+BEGIN_SRC java
void inOrder(TreeNode root) {
    Stack <TreeNode> s = new Stack <>();
    TreeNode current = root;
    s.push(root);

    while(current != null || !s.isEmpty()) {
        while(currnet != null) {
            current = current.left;
            s.push(current);
        }
        current = s.pop();
        System.out.print(s.data);
        current = current.right;
    }
}
#+END_SRC
*** Range Sum of BST
Given the root node of a binary tree, return the sum of values of all nodes with value between L and R (inclusive). The binary search tree is guaranteed to have unique values.

Example 1:
I: root = [10,5,15,3,7,null,18], L = 7, R = 15
O: 32

The first thing that comes to mind is use any traversal method to visit each node. If the value of the node is between L and R (inclusive) we add it to the sum. When we have finished traversing the tree, we return the sum.

#+BEGIN_SRC java
// Recursive implementation

// Class variable
int sum = 0;
int rangeSum(TreeNode root, int L, int R) {
    return getSum(root, sum, L, R);
}

void getSum(TreeNode root, int L, int R) {
    if(root != null) {
        getSum(root.left, L, R);
        if(L <= root.data && root.data <= R) {
            sum += root.data;
        }
        getSum(root.right, L, R);
    }
}
#+END_SRC

We can also do the same with an iterative tree traversal using a stack
#+BEGIN_SRC java
int rangeSum(TreeNode root, int L, int R) {
    Stack <TreeNode> s = new TreeNode <>();
    TreeNode current = root;
    int sum = 0;

    while(current != null || !s.isEmpty()) {
        while(current != null) {
            s.push(current);
            current = current.left;
        }
        current = s.pop();
        if(L <= current.val && current.val <= R) {
            sum += current.val;
        }
        current = current.right;
    }
    return sum;
}
#+END_SRC
*** Merge Two Sorted Lists
Consider two singly linked lists in which each node holds a number. Assume the lists are sorted. The merge of the two lists is a list consisting of the nodes of the two lists in which numbers appear in ascending order.

Similar to how we merge two sorted arrays, we use two pointers one for the L1 (list 1) and L2 (list 2). We iterate through the lists. We take the node with the smaller value and advance the pointer up by one. When we have reached the end of one list we append the remaining list to the end.

#+BEGIN_SRC java
Node mergeList(Node L1, node L2) {
    Node newHead = new Node(0);
    Node newCurr = newHead, curr1 = L1, curr2 = L2;

    while(curr1 != null && curr2 != null) {
        if(curr1.data < curr2.data) {
            newCurr.next = curr1;
            curr1 = curr1.next;
        } else {
            newCurr.next = curr2;
            curr2 = curr2.next;
        }
        newCurr = newCurr.next;
    }

    newCurr.next = curr1 == null ? curr2 : curr1;
    return newHead.next;
}
#+END_SRC
*** Reverse a Single Sublist
Write a program which takes a singly linked list L and two integers s and f as arguments, and reverses the order of the nodes from the sth node to fth node, inclusive. The numbering begins at 1. Do not allocate additonal nodes.

There are two ways to solve this problem that rely on the same premise. If we are to reverse a linked list from the sth node to the fth node we need to reach that node first.

#+BEGIN_SRC java
Node current = head;
int counter = 1;
while(counter++ < s) {
    current = current.next;
}
#+END_SRC

Once we have found the node to begin reversing from, the variation in how we implement our answer shows itself.
The brute force method would be to keep track of the previous node as well when we find the node to begin reversing. This is so that we have an idea of where our new "subhead" is going to be.

We then begin reversing how you would normally reverse a linked list, except we stop until the counter is at f. We also assign a pointer, let's call it continueNode, to prev. We also assign a variable called tail to curr, as this will be where our list ends since it will be reversed.
#+BEGIN_SRC java
while(counter++ < f) {
    Node next = current.next;
    current.next = prev;
    prev = current;
    current = next;
}
#+END_SRC


After reversing the sublist we have two pointers: continueNode which points to the "subHead" and tail which is the end of the reversed sublist. Current is also at node after f, or f + 1. We now simply append pointers such that the list is correctly connected.

#+BEGIN_SRC java
// If the continueNode is not null then we set its next node to prev which is the new subHead.
if(continueNode != null) {
    continueNode.next = prev;
}
// If continueNode is null then that meanst the head of the list was also reversed.
else {
    head = prev;
}

// Append the end of the reversed sublist to f + 1.
tail.next = curr;
#+END_SRC

The implementation is below
#+BEGIN_SRC java
Node reverseSublist(Node head, int s, int f) {
    if(head == null) return null;
    Node curr = head, prev = null;
    int counter = 1;

    while(counter++ < s) {
        prev = curr;
        curr = curr.next;
    }

    Node tail = curr, continueNode = prev, next = null;
    while(counter++ < f) {
        next = curr.next;
        curr.next = prev;
        prev = curr;
        curr = next;
    }

    if(continueNode != null){
        continueNode.next = prev;
    } else {
        head = prev;
    }

    tail.next = curr;
    return head;
}
#+END_SRC

There is another implementation that is more elegant than the previous solution. The idea is to take advantage of the how reversing a list works. For example let's say that we are given the list:
[head] -> [1: next] -> [2:next] -> [3:next] -> [4:tail] -> null

If we are to reverse the list traditionally we would have an iterator, usuall called current, and a previous pointer and swap pointers until the list is reversed. However another way to reverse the list is to swap iterator pointers and next pointers. We need a dummy node, just incase the reversal of the list includes the first node of the list. Let's reverse the list from the example.

[head] -> [0:next:Dummy] -> [1:next:Iterator] -> [2:next:Next] -> [3:next] -> [4:tail] -> null
Swapping iterator's next pointer so that it points to Next's next so that we now have [1:next:Iterator] -> [3:next]. We then set Next's next pointer to Dummy's next [2:next:Next] -> [1:next:Iterator]. Finally we swap Dummy's next pointer so that it points to Next. [0:next:Dummy] -> [2:next:Next]. The list now looks like this: [head] -> [0:next:Dummy] -> [2:next:Next] -> [1:next:Iterator] -> [3:next] -> [4:tail] -> null.

What we have done is moved the iterator forward by putting the node next to the iterator to the start of the sublist. The next iteration of the reversal would see node 3 moved to the front such that the list's values read 3 -> 2 -> 1 -> 4. Implementing this is not very difficult.

#+BEGIN_SRC java
Node reverseSublist(Node head, int s, int f) {
    if(head == null) return null;
    Node dummyHead = new Node(0);
    dummyHead.next = head;
    head = dummyHead;
    Node dummyCurr = dummyHead;

    int k = 1;
    while(k++ < s) {
        dummyCurr = dummyCurr.next;
    }

    Node Iterator = dummyCurr.next;
    while(s++ < f) {
        Node Next = Iterator.next;
        Iterator.next = Next.next;
        Next.next = dummyCurr.next;
        dummyCurr.next = Next;
    }
    return dummyHead.next;
}
#+END_SRC
*** Test for Cyclicity
Write a program that takes the head of a singly linked list and returns null if there does not exist a cycle, anda  node at the start of the cycle, if a cycle is present.

Detecting a cycle in a linked list is fairly straight forward. Using floyd's algorithm we can determine if a list has a cycle in linear time. The difficulty of this problem lies in how we determine the starting node of the cycle. There are two ways we can determine the start of the cycle.

The first way is to find the size of the cycle. This can be done by keeping one of the pointers in the cycle and moving the other pointer counting each unique node it encounters until it finds the static pointer again. Once we have found the size of the cycle, we set one of the pointers to the head and move it forward the same value as the size of the loop. We then initialize a pointer to the head of the list again and continue to move both pointers until they land on each other. When they intersect that is the node that is the start of the cycle.

#+BEGIN_SRC java
Node getLoop(Node head) {
    Node slow = head, fast = head;
    while(fast != null&& fast.next != null) {
        fast = fast.next.next;
        slow = slow.next;
        if(slow == fast) {
            int count = 1;
            slow = slow.next;
            while(slow != fast) {
                slow = slow.next;
            }

            slow = head;
            fast = head;
            while(count-- > 0) {
                slow = slow.next;
            }

            while(slow != fast) {
                slow = slow.next;
                fast = fast.next;
            }

            return slow;
        }
    }
    return null;
}
#+END_SRC
*** Test for overlapping lists
Given two singly linked lists there may be list nodes that are common to both. Write a program that takes two cycle-free singly linked lists, and determines if there exists a node that is common to both lists.

There are two ways to solve this problem. The first way is to brute force the problem. We create a hashmap and begin pushing nodes from one list into the map. We then iterate through the second list, checking at every node if it is present in the hashmap. If it is present, we return that node. This is a O(nk) and O(n) solution, where n is the length of one list and k is the length of the second list.

A more elegant solution to this problem is to get the length of both lists. After getting the lengths of both lists, we initialize a pointer to the start of the longer list and continue iterating until the difference of the lengths for both lists is 0. We then initialize a pointer to the other list and iterate both pointers until they either intersect or terminate at null. If they intersect that node is the overlapping node, of they termiante at null there is no overlap.

#+BEGIN_SRC java
Node overlappingLists(Node A, Node B) {
    int countA = 0, countB = 0;
    Node currA = A, currB = B;

    while(currA != null) {
        currA = currA.next;
        countA++;
    }

    while(currB != null) {
        currB = currB.next;
        countB++;
    }

    Node fast = countA > countB ? A : B;
    Node slow = countB > countA ? A : B;
    int longerList = count > countB ? A : B;

    while(fast != slow && fast != null && slow != null) {
        fast = fast.next;
        slow = slow.next;
        if(fast == slow) {
            return slow;
        }
    }
    return null;
}
#+END_SRC
*** Test for overlapping lists - lists my have cycles
Solve the previous problem, with the addition that a list or bot lists may have a cycle. You may return any node that appears in their overlap.

Let us take a moment to consider a case where there is an overlap. If there is to be an overlap for these two lists where a cycle may be present, both lists must have a cycle. We start with a check to see if both lists have a cycle. If either list does not have a cycle, we can return null as there is no overlap. If both lists do have a cycle, if there is an overlap, the cycle must be identical.

We can return a node in the overlap by setting one of the pointers back to the start of either list. We then traverse the node until we encounter
*** Delete A Node from a Singly Linked List
Write a program that which deletes a node in a singly linked list. The input node is guranteed not to be the tail node.

The only input we are given is the node that we must delete. Due to the fact that it is singly linked, we do not have access to a previous pointer to remove the current node from the list. What we can do is copy the next node's data into the current node. After copying the data over, we set the next pointer to the nodes next's next pointer. We must then set next's next pointer to null to disconnect it from the list to free up memory.

#+BEGIN_SRC java
void deleteNode(Node delete) {
    delete.data = delete.next.data;
    Node next = delete.next;
    delete.next = next.next;
    next.next = null;
}
#+END_SRC
*** Remove the kth last element from a list
Given a singly linked list and an integer k, write a program to remove the kth last element. Your algorithm cannot use more than a few words of storage, regardless of the length of the list.

There are two different algorithms we can implement here. The first way is to simply iterate through the list to get the length of the list. After getting the length of the list we get the difference of the length and k. Then iterate through the list (length - k) times.

The second way is to take advantage of the fact that we can create a buffer. Initialize two pointers at the head of the list. Iterate one of the pointers k times. After iterating that pointer forward k times, begin iterating both pointers one at a time until the fast pointer reaches the end of the list. Return the slow pointer.

Implementation 1
#+BEGIN_SRC java
Node kthLast(Node head, int k) {
    Node slow = head;
    int length = 0;
    while(slow != null) {
        length++;
        slow = slow.next;
    }

    slow = head;
    while((length - k)-- > 0) {
        slow = slow.next;
    }
    return slow;
}
#+END_SRC

Implementation 2
#+BEGIN_SRC java
Node kthLast(Node head, int k) {
    Node slow = head, fast = head;
    while(k-- > 0) {
        fast = fast.next;
    }

    while(fast != null) {
        slow = slow.next;
        fast = fast.next;
    }

    return slow;
}
#+END_SRC
*** Remove Duplicates from a Sorted List
Write a program that takes as input a singly linked list of integers in sorted order, and removes duplicates from it.

There are two ways to solve this problem. The first way is to create a hashset and then push all values from the list into the list. We then create a new list and test if a value has already been added to the set. If it has not we append that node to the new list. A better way to do this is to use a fast pointer that begins at every unique value.

As we move forward with the fast pointer we continue as long as the value matches the slow pointer. When we encounter a node that is not the same as the slow pointer, we append the slow.next to fast.

#+BEGIN_SRC java
void deleteDuplicates(Node head) {
    Node curr = head;
    while(curr != null) {
        Node iter = curr.next;
        while(curr.data != iter.data && iter != null) {
            iter = iter.next;
        }
        curr.next = iter;
        curr = iter;
    }
}
#+END_SRC
*** Implement a Cyclic Right Shift for Singly Linked lists
Write a program that takes as input a singly linked list and a nonnegative integer k, and returns the list cyclically shifted to the right by k.

There are two ways to implement a solution for this problem. The first way is to write a subroutine that transplants the tail to the head. We then call that subroutine k times.

The second way is to fing the kth node from the end and k-1 node from the end. Since the kth node from the end will be our new head, we disconnect it from our list, and then append the tail to our head. We then return the kth node as our new head.

#+BEGIN_SRC java
Node cyclicShift(Node head, int k) {
    while(k-- > 0) {
        head = transplantTail(head);
    }
    return head;
}

Node transplantTail(Node head) {
    Node curr = head, prev = null;;
    while(curr.next != null) {
        prev = curr;
        curr = curr.next;
    }

    prev.next = null;
    curr.next = head;
    return curr;
}
#+END_SRC

#+BEGIN_SRC java
Node cyclucShift(Node head, int k) {
    Node slow = head, fast = head;
    while(k-- > 0) {
        fast = fast.next;
    }

    while(fast.next != null) {
        slow = slow.next;
        fast = fast.next;
    }

    fast.next = head;
    slow.next = null;
    return head;
}
#+END_SRC
*** Implement Even-Odd Merge
Consider a singly linked list whose nodes are numbered starting at 0. Define the even-odd merge of the list to be the list consisting of the even-numbered nodes followed by the odd-numbered nodes. Write a program that computes the even-odd merge.

There are two seperate implementations that immediately come to mind. The first implementation is a brute force solution where we create two seperate lists. This is an O(n) time and memory solution.

The better solution is to just redircet the pointers to save memory.

#+BEGIN_SRC java
Node evenOdd(Node head) {
    Node evenHead = new Node(-1);
    Node oddHead = new Node(-1);

    Node curr = head, evenCurr = evenHead, oddCurr = oddHead;
    while(curr != null) {
        if(curr.data % 2 == 0) {
            evenCurr.next = new Node(curr.data);
            evenCurr = evenCurr.next;
        } else {
            oddCurr.next = new Node(odd.data);
            oddCurr = oddCurr.next;
        }
        curr = curr.next;
    }

    evenCurr.next = oddHead.next;
    return evenHead.next;
}
#+END_SRC

#+BEGIN_SRC java
Node evenOdd(Node head) {
    Node evenHead = new Node(-1);
    Node evenHead = new Node(-1);

    Node curr = head, evenCurr = evenHead, oddCurr = oddHead;
    while(curr != null) {
        Node Next = curr.next;
        if(curr.data % 2 == 0) {
            evenCurr.next = curr;
            evenCurr = evenCurr.next;
        } else {
            oddCurr.next = curr;
            oddCurr = oddCurr.next;
        }
        curr.next = null;
        curr = Next;
    }
    evenCurr.next = oddHead.next;
    return evenHead.next;
}
#+END_SRC
*** Test Whether a Singly Linked List is Palindromic
Write a program that tests whether a singly linked list is palindromic.

There are two ways to solve this problem. The first ways it to brute force make comparisons by iterative twice for every node. Compare the first node to the last node. First + 1 to the Last - 1 node and so on. A more elegant solution would be to reverse the list from the middle of the list and make comparisons. Irregardless of whether the lists are equal or not, we then unreverse the sublist and reappend it to the original list.

#+BEGIN_SRC java
boolean isListPalindrome(Node head) {
    Node slow = head, fast = head;
    while(fast != null && fast.next != null) {
        slow = slow.next;
        fast = fast.next.next;
    }

    Node firstHalf = head, secondHalf = reverseList(slow);
    while(secondHalf != null && firstHalf != null) {
        if(secondHalf.data != firstHalf.data) {
            return false;
        }
        secondHalf = secondHalf.next;
        firstHalf = firstHalf.next;
    }
    return true;
}
#+END_SRC
*** Implement List Pivoting
For any integer K, the pivot of a list of integers with respect to k is that list with its node reordered so that all nodes containing keys less than k appear before nodes containing k, and all nodes containing keys greater than k appear after the nodes containing k. Implement a function which takes as input a singly linked list and an integer k and performs a pivot of the list with respect to k.


There are two implementations for this problem. The first implementation is to create three seperate lists. Iterate through the original list and create new nodes appending them to the appropriate lists based on if the node is less than k, equal to k, or greater than k. A better implementation follows a similar method to the even-odd merge solution. Instead of creating new nodes, we create three dummy nodes and reorganize pointers such that we create three sublists that satisfy the conditions for k in one pass. We then append them together and return the head of the list.

#+BEGIN_SRC java
Node listPivoting(Node head, int k) {
    Node lessThan = new Node(-1);
    Node equalTo = new Node(-1);
    Node greaterThan = new Node(-1);

    Node lessPointer = lessThan;
    Node equalPointer = equalTo;
    Node greaterPointer = greaterThan;

    Node iterator = head;
    while(iterator != null) {
        if(iterator.data < k) {
           lessPointer.next = iterator;
           lessPointer = iterator;
        } else if(iterator.data == k){
            equalPointer.next = iterator;
            equalPointer = iterator;
        } else {
            greaterPointer.next = iterator;
            greaterPoint = iterator;
        }
        iterator = iterator.next;
    }
    lessPointer.next = equalTo.next;
    equalPointer.next = greaterThan.next;
    return lessThan.next;
}
#+END_SRC
*** Test if a Binary Tree is Height-Balanced
Write a program that takes as input the root of a binary tree and checks whether the tree is height-balanced. A height-balanced tree means that that the difference in the height of its left and right subtrees is at most one.

The problem seems slightly intimidating until we look at the properties we can take advantage of. Essentially we are checking if the absolute difference of the heights of two trees is greater than 1. We can find the height of subtrees the same way we find the height of a tree. We recursively check for the height of a tree by performing a null check and then adding 1. The only difference is we are not returning the maximum depth of the tree.

#+BEGIN_SRC java
class BalancedTree {
    boolean isBalanced = true;

    public boolean isTreeBalanced(TreeNode root) {
        helper(root);
        return isBalanced;
    }

    public int helper(TreeNode root) {
        if(root == null) {
            return 0;
        }

        int L = helper(root.left);
        int R = helper(root.right);

        if(Math.abs(L - R) > 1) {
            isBalanced = false;
        }

        return Math.max(L, R) + 1;
    }
}
#+END_SRC
*** Test if a Binary Tree is Symmetric
A binary tree is symmetric if you can draw a vertical line through the root and then the left subtree is the mirror image of the right subtree. Write a program that checks whether a binary tree is symmetric.

There are two things we need to check for a binary tree to be symmetric. Are the value of the nodes equal and are they mirrored. We can check if nodes are mirrored with a null check. We simply make recursive calls down the tree.

#+BEGIN_SRC java
boolean isTreeSymmetric(TreeNode root) {
    if(root == null) {
        return true;
    }
    return helper(root.left, root.right);
}

boolean helper(TreeNode L, TreeNode R) {
    if(L == null && R == null) {
        return true;
    } else if(L != null && R != null) {
        return L.data == R.data && helper(L.left, R.right) && helper(L.right, R.left);
    }
}
#+END_SRC
*** Find a Root to Leaf Path With Specified Sum
You are given a binary tree where each node is labaled with an integer. Tha tpath weight of a node in such a tree is the sum of the integers on the unique path from the root to that node. Write a program which takes as input an integer and a binary tree with integer node weights, and checks if there exists a leaf whose path weight equals the given integer.

It is important to note that this is not a binary search tree (BST). This means that there is no order or searching property that we can take advantage of. Rather what we need to do is check every possible route. We can do this by starting from the root and recursively traversing down the tree.

If the root is null then there is no path that adds up to the sum. When we recurse down left and right, we must subtract that value from the target value. We continue until we reach a leaf node, because we are looking for a full path, not a partial path.

#+BEGIN_SRC
doesPathSumExist(Root, k)
    if Root = nil do
        return false
    else if Root.left != nil AND Root.right != nil do
        return Root.data = K
    return doesPathSumExist(Root.left, k - Root.data) OR doesPathSumExist(Root.right, k - Root.data)
#+END_SRC
*** Compute the Kth node in an In-Order Traversal
Write a program that efficiently computes the kth node appearing in an inorder matraversal.

There are a few ways to approach this problem. The first way is to perform an inorder recursive traversal, counting k down from 1 every time we traverse a node. The second way is to perform an interative recursive traversal, counting down again.

The optimal solution is a little bit more elegant. Let us assume we are given a tree [1,2,3,4,5,null,null,6,7,8] and k of 5. The fifth element of an inorder traversal would be 8.

Let us again assume that the TreeNode class has an atrribute called .size which is the size of the tree. We can take advantage of this attribute to find the kth node.

First let us get the size of the left subtree of the root. In the above example the size of our left subtree is 6 nodes. Add 1 to 6 as we are including the root in our search we have a total of 7 nodes. Since 5 < 7, we know that the left subtree including the root contains the 5th node. So we then traverse to root.left.

Again we check the size of the left subtree. The size is 3, adding one for our root the total size is 4. 4 is less than 5, or the size of the left subtree plus the root is less than k. This means that the kth node is in our right subtree. Do not forget to subtract the size of the left subtree plus root.

k is now 1. Again we look for the size of the left subtree plus the root which is 2. 1 < 2 meaning that the kth node is contained in our left subtree.

The size of our left subtree is now 0 since we are at a leaf. If the size of our left subtree is equal to k - 1, we have found the kth node in an inorder traversal.

Note that this algorithm runs significantly faster only if the TreeNode class has the size attribute. If the TreeNode class does not have the size attribute and we must implement a size subroutine then the time complexity is the same as the brute force.

#+BEGIN_SRC
KthNodeTree(Root, k)
    if k = 0 do
        return Root
    else if Root != null do
        KthNodeTree(Root.left, k - 1)
        KthNode(Root.right, k - 1)
#+END_SRC

#+BEGIN_SRC
KthNodeTree(Root, k)
    if Root = null do
        return null
    TreeNode current <-- Root
    while current != null do
        leftTreeSize <-- current.left.size
        if leftTreeSize + 1 < k do
            k <-- k - (leftTreeSize + 1)
            current <-- current.right
        else if leftTreeSize = k - 1 do
            return current
        else
            current <-- current.left
    return null
#+END_SRC
*** Compute the Lowest Common Ancestor in a Binary Tree
Design an algorithm for computing the LCA of two nodes in a binary tree in which nodes do not have a parent field.

Given any two nodes in a tree, they share an ancestor: the root. The lowest common ancestor is the lowest node where two nodes share the node as a subroot. A naieve solution would be to write a subroutine that checks if a tree contains the two nodes. You would then call that subroutine in a post order traversal.

A better solution would be to create a class that has two attributes. One that tells us how many of the two nodes are found and the root of the tree. We then start from the leaves of the tree and work our way up. If the left subtree's object has a value of 2 then we return it. If the right subtree's value has a value of 2 then we return it. If either are not two we take the sum of both counts in addition to checking if the root equals either the left child or right child and adding 1 if either is true.

#+BEGIN_SRC
LowestCommonAncestor(Root, x, y)
    if root = null do
        return null
    leftRoot <-- LowestCommonAncestor(Root.left, x, y)
    if leftRoot.count = 2
        return leftRoot
    rightRoot <-- LowestCommonAncestor(Root.right, x, y)
    if rightRoot.count = 2
        return rightRoot
    numNodes <-- leftRoot.count + rightRoot.count + (root = x ? 1 : 0) + (root = y ? 1 : 0)
    return new object(numNodes, numNodes = 2 ? root : null)
#+END_SRC
*** Compute the Lowest Common Ancestor in a Binary Tree (With Parent Attribute)
Design an algorithm for computing the LCA of two nodes in a binary tree.

If we have access to the parent node then we must consider two cases. We must consider the first case where both node x and y are the same depth and the second case where x and y are of different heights.

If both x and y are at the same height we traverse up the tree until they intersect. The intersection is the ancestor node. If x and y are at different depths then find the deepest node. Then increment the deepest node until it is at an even height with the other node. Then we travel up the tree until they intersect.

#+BEGIN_SRC
LowestCommonAncestorParent(x, y)
    xHeight <-- maxDepth(x)
    yHeight <-- maxDepth(y)


    if yHeight > xHeight do
        temp <-- x
        x <-- y
        y <-- temp

    depthDiff <-- Absolute(yHeight - xHeight)
    while depthDiff > 0 do
        depthDiff <-- depthDiff - 1
        x <-- x.parent

    while y != x do
        y = y.parent
        x = x.parent

    return x
#+END_SRC
*** Find the In-Order Successor
Design an algorithm that finds the inorder successor for a node k. You have access to a parent attribute.

An inorder successor is the node that appears next in an inorder traversal. We could brute force the solution by calling an inorder traversal, but there is a more elegant solution than this.

Since the inorder traversal is a DFS, we must first check if the node k has a right child. If the node k does have a right child, we simply return the left most leaf for k.right. The problem becomes more tricky if k does not have a right child. If k does not have a right child, we need to traverse back up the tree. If k does not have a right child we know that there are two conditions that need to be met for the successor node.

1. If k is not a successor node, it is a right child.
2. k is not null

Using these two givens, we can come up with the condition that if k is not null and k is not a right child then the parent of k is the successor.

#+BEGIN_SRC
GetSuccessor(k)
    if k.right != null do
        return getMinimum(k.right)
    else
        parent <-- k.parent
        while k != null AND k = parent.right do
            k <-- parent
            parent <-- parent.parent
        return parent
#+END_SRC
*** Find the In-Order Successor (BST with no Parent)
Similar to finding the inorder successor for a binary tree the catch for this problem is that we do not have access to the parent attribute and that the tree is a binary search tree.

The inorder successor has an additional implication in a binary search tree. The inorder successor is still the next node in an inorder traversal, but it is also the smallest element that is larger than the node k.

We can take advantage of the fact that the tree is a BST to find the successor. Rather than working up the tree as we did with a parent attribute, we will be working down the tree in this instance.

Using the important property from before, where we identified that if k is not a succesor it is a right child we can set up a recursive function that works its way down from the tree to find the successor.

#+BEGIN_SRC
GetSuccessor(root, succ, k)
    if root = null do
        return null
    if root.data = k do
        if root.right != null do
            return getMinimum(root.right)
    else if root.data < k do
        succ <-- root
        return getSuccessor(root.left, succ, k)
    else
        return getSuccessor(root.right, succ, k)
    return succ
#+END_SRC
*** Merge Two Binary Trees
Given two binary trees and imagine that when you put on of them to cover the other, some nodes of the two trees are overlapped while the others are not. You need to mreger them. The merge rule is that if two nodes overlap, then sum node values up as the new value of the merged node. Otherwise the NOT null node will be used as the node of new tree.

We can solve this problem with a recursive approach. We know that if both nodes exist then we simply add their values. If a node in treeOne does not exist and a node in treeTwo does, we return treeTwo. The vice versa applies.

#+BEGIN_SRC
MergeTrees(TreeOne, TreeTwo)
    if TreeOne = null do
        return TreeTwo
    if TreeTwo = null do
        return TreeOne
    TreeOne.data <-- TreeOne.data + TreeTwo.data
    TreeOne.left <-- MergeTrees(TreeOne.left, TreeTwo.left)
    TreeTwo.right <-- MergeTrees(TreeTwo.right, TreeTwo.right)
    return TreeOne
#+END_SRC

We can also solve this problem using a stack. To replicate two tree traversals, we push an array consisting of the roots of TreeOne and TreeTwo. We then begin looping as long as the stack is not empty. First pop the array of roots off the stack. If either root is null we then continue to the next cycle of the list.

Add TreeTwo.data to TreeOne.data if both roots exist. After we add values we need to check for their children. If TreeOne.left is null then we know that TreeOne.left now must take TreeTwo.left. If TreeOne.left is not null then we need to merge TreeOne.left and TreeTwo.left as well.

If TreeOne.right is null then we append TreeTwo.right to TreeOne.right. If TreeOne.right is not null then we need to perform another merge operation and merge TreeOne.right and TreeTwo.right.

When we exit the loop the stack is empty and the trees have been merged.

#+BEGIN_SRC
MergeTrees(TreeOne, TreeTwo)
    Stack <-- New Stack
    Stack.push(Array(TreeOne, TreeTwo))
    while Stack.size != 0 do
        TreeArr <-- Stack.pop()
        if TreeArr[1] = null OR TreeArr[2] = null do
            continue
        TreeArr[1] <-- TreeArr[1] + TreeArr[2]
        if TreeArr[1].left = null do
            TreeArr[1].left <-- TreeArr[2].left
        else do
            Stack.push(Array(TreeArr[1].left, TreeArr[2].left))
        if TreeArr[1].right = null do
            TreeArr[1].right <-- TreeArr[2].right
        else do
            Stack.psuh(Array(TreeArr[1].right, TreeArr[2].right))
    return TreeOne
#+END_SRC
***  Search in a Rotated Array
Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. You are given a target value to search. If found in the array return its index, otherwise return -1. You may assume no duplicate exists in the array. You algorithm's complexity must run in the order of O(log n).

We can solve this problem by breaking the array into two subarrays where we have two sorted arrays. Assuming k is the pivot we have A[1...k] and A[k + 1...A.length]. The problem here lies whether we can find the pivot k in logarithmic time. Finding it in linear time is trivial as we search for an element where A[k - 1] < A[k] > A[k + 1].

To find it in logarithmic time we need to take advantage of the somewhat sorted input.

Looking at the array <4,5,6,7,0,1,2> we know that the pivot is 0. We know that the pivot is 0, because it is a peak element. We also know that if the element at the middle of the array is less than the last element, then the pivot is in the right half. If the middle element is less than the last element than the pivot is in the left half.

We know we have found the pivot if one of three conditions are met.
    1. If A[mid] > A[mid + 1]
    2. If A[mid] < A[mid - 1]
    3. If the size of the array is one.

If A[mid] > A[mid + 1] then A[mid] is the pivot.
If A[mid] < A[mid - 1] then A[mid - 1] is the pivot.
If the size of the array is 1 then there are no other possible choices.

#+BEGIN_SRC
FindPivot(A, l, r)
    if l = r do
        return l
    mid <-- (l + r) / 2
    if A[mid] > A[mid + 1] do
        return mid
    if A[mid - 1] > A[mid] do
        return mid - 1
    if A[mid] > A[r] do
        return FindPivot(A, mid + 1, r)
    else
        return FindPivot(A, l, mid - 1)
#+END_SRC

Now how can we take advantage of knowing where the pivot is? Well if we are looking for a value x in an array A and we have the value of the pivot A[k] we now which side we need to perform binary search on. If A[k] > x then we perform binary search on A[1...k]. If A[k] < x then we perform binary search on A[k...A.length].

#+BEGIN_SRC
RotateBinarySearch(A, x)
    pivot <-- FindPivot(A, 1, A.length)
    if A[pivot] = x do
        return pivot
    if A[0] <= x do
        return BinarySearch(A, 1, pivot - 1, x)
    else
        return BinarySearch(A, pivot + 1, A.length, x)
#+END_SRC
**** Variant A
A sequence is strictly ascending if each element is greater than its predecessor. Suppose it is known that an array A consists of a strictly ascending sequence followed by a strictly descending sequence. Design an algorithm for finding the position of an element k in a cyclically sorted array of distinct elements.

Get the median of the array. If A[median] < A[median - 1] AND median != 0 then we recurse to the left half of the array. if A[median + 1] > A[median] AND median != A.length - 1 then we recurse to the right half of the array. If A[m] > A[m - 1] AND A[m] > A[m + 1] return A[m].
*** Search a sorted array for first instance of k
Binary search commonly asks for the index of any element of a sorted array that is equal to a specified element. Write a method that takes a sorted array and a key and returns the index of the first occurence of the first occurrence of that key in the array, Return -1 if the does not appear in the array.

We can brute force this problem in linear time by iterating from the beginning of the array until we find k. However this does not take advantage of the sorted property of the array. What we can do to take advantage is to modify the binary search algorithm. We modify it so that if A[mid] equals k and A[mid - 1] is equal to A[mid] then we recursively call the program to the left half.

#+BEGIN_SRC
FirstK(A, l, r, k)
    if(r >= l)
        m <-- l + (r - l) / 2
        if A[m] = k AND (m = 0 OR A[m] > A[m - 1]) do
            return m
        if A[m] > k do
            return FirstK(A, l, m - 1, k)
        else
            return FirstK(A, m + 1, r, k)
    return -1
#+END_SRC
**** Variant A
Design an efficient algorithm that takes a sorted array and a key and finds the index of the first occurrence of an element greater than that key.

The problem sounds somewhat complicated, but the solution is fairly simple if we think about it. Rather than focusing on what the base case is for our recursion, we focus on finding the cases for when we recurse. If given a value k and it is equal to k, we need to recurse to the right side since we are looking for an element greater than k. If an element at m is less than k, we need to recurse to the right half. If an element at m is greater than k, we need to recurse to the left half.

Where k = A[m] and A[m] < k both share the same condition to recurse.

Our base case is when the size of the array is 1 or when left = right.

#+BEGIN_SRC
SearchGreaterThanK(A, l, r, k)
    if l <= r do
        m <-- l + (r - l) / 2
        if l = r do
            return A[m]
        if A[m] = k OR A[m] < k
            return SearchGreaterThanK(A, m + 1, r, k)
        else
            return SearchGreaterThanK(A, l, m - 1, k)
    return -1
#+END_SRC
**** Variant C
Write a program which takes a sorted array A of integers, and an integer k, and returns the interval enclosing k, the pair of integers L and U such that L is the first occurrence of k in A and U is the last occurrence of k in A. If k does not appear in A, return [-1, -1].

Let us say the input we are given is <1,2,2,4,4,4,7,11,11,13> and k = 11. We shoudl return 7,8 as they are the indices that enclose 11. The brute force way to solve this problem would be to just perform a linear scan looking for the first instance of k and then the last instance of k. This obviously takes O(n) time.

We can reduce the time complexity of this algorithm to O(log n) by finding the first occurrence of k and the last occurrence of k. We then return both indices as an array.

Find first instance of k
#+BEGIN_SRC
FirstK(A, l, r, k)
    if(l <= k)
        m <-- l + (r - l) / 2
        if A[m] = k AND (m = 0 OR A[m] > A[m - 1]) do
            return m
        if A[m] > k do
            return FirstK(A, l, m - 1, k)
        else
            return FirstK(A, m + 1, r, k)
    return -1
#+END_SRC

Find last instance of k
#+BEGIN_SRC
FindLastK(A, l, r, k)
    if(l <= k)
        m <-- l + (r - l) / 2
        if A[m] = k AND (m = 0 OR A[m] < A[m + 1]) do
            return m
        if A[m] > k do
            return FindLastK(A, l, m - 1, k)
        else
            return FindLastk(A, m + 1, r, k)
    return -1
#+END_SRC

Get Enclosure
#+BEGIN_SRC
GetEnclosure(A, l, r, k)
    first <-- FirstK(A, 1, A.length, k)
    last <-- FindLastK(A, 1, A.length, k)
    if first = -1 OR last = -1 do
        return -1
    return new Array(first, last)
#+END_SRC

*** Search Entry Equal to Index
Design an efficient algorithm that takes a sorted array of distinct integers and returns and index i such that the element at index i equals i.

Let us say we are given the array <-2,0,2,3,6,7,9>. We are expected to return either 2 or 3 for this algorithm. Finding the solution in linear time is a trivial feat: we iterate through the array until we find the first instance where the index matches the element.

We can get our time complexity time to O(log n) using binary search. The big question here is what property will help us to use binary search? Looking at the array <-2,0,2,4,6,7,8> take note that the only possible anwer is index 2. If we get the element at the mid point (l + (r - l) / 2) we get the value 4 at index 3. This means that every element after index 3 at least has a difference of 1. If an element does have a search entry equal to its index then it must be before the mid point. The reverse logic applies as well.

#+BEGIN_SRC
SearchEntryEqualIndex(A, l, r)
    if l <= r do
        m <-- l + (r - l) / 2
        if A[m] = m do
            return m
        else if A[m] > A[m] - m do
            SearchEntryEqualIndex(A, l, m - 1)
        else
            SearchEntryEqualIndex(A, m + 1, r)
    return -1
#+END_SRC
**** Variant
Solve the same problem when A is sorted but may contain duplicates.

The variant does not particularly change the problem. Since we are looking at the difference between A[m] and m, duplicates do not affect the way the algorithm functions.
*** Running Sum of 1d Array
Given an array nums we define a running sum of an array as runningSum[i] = sum(nums[0]...nums[i]).

We initialize a new array with the same size as nums. The first element in this new array is nums[0]. The second element is nums[0] + nums[1]. After nums[1], every answer[i] is nums[i] + answer[i - 1].

#+BEGIN_SRC
f(A)
    Answer <-- new Array(A.length)
    Answer[0] <-- A[0]
    Answer[1] <-- A[0] + A[1]
    for i <-- 2 to A.length - 1 do
        Answer[i] <-- A[i] + Answer[i - 1];
    return Answer
#+END_SRC
*** Dutch National Flag Problem
Write a program that takes an array A and an index i into A, and rearranges the elements such that all elements less than A[i], appear first, followed by elements equal to the pivot, followed by elements greater than the pivot.

I: A: <0,1,2,0,2,1,1>, P: 3
O: <0,0,1,2,2,1,1> OR <0,0,1,1,1,2,2>

The brute force is to perform two passes on the array. We can "grow" the array from the front and back of the array. Initalize two pointers, one called less and one called greater. Less is initalized with a value of 1 and greater is initialized with a value of A.length.

On the first pass of the array, we start from the front of the array. We are looking to find elements that are less than A[p] and swap them into the correct half of the array. If A[i] < A[p] then we swap the values at A[i] and A[smaller] and increment smaller by one.

On the second pass we start from the back of the array and iterate to the front. We are looking for elements that are greater than A[p]. If A[p] is less than A[j] then we swap the values at A[j] and A[greater] and then decrement greater by one.

#+BEGIN_SRC
f(A, p)
    smaller <-- 1
    larger <-- A.length
    pivot <-- A[p]
    for i <-- 1 to A.length do
        if A[i] < A[p] do
            swap A[smaller] and A[i]
            smaller <-- smaller + 1
    for j <-- A.length to 1 do
        if A[i] > pivot do
            swap A[larger] and A[j]
            larger <-- larger - 1
#+END_SRC

Bottlenecks:
    1. 2 passes make the run time longer than it needs to be.

We can do this in 1 pass by manipulating pointers a little bit. Picture the input array A as consisting of 4 subarrays. Subarray that has elements less than A[p], elements equal to A[p], elements greater than A[p], and elements we are not sure yet. We use three pointers, less, equal, and greater. Greater starts at the end of the array and both less and equal start at the front of the array.

The idea is to use equal as both the last index pointer for equal elements and iterator. We will iterate until equal and greater cross paths. If we find an element that is less than A[p], we swap A[less] and A[equal] and then increment less and equal by one. If we find an element that is equal to A[p], we simply increment the equal pointer. If we find an element greater than A[p], we swap with A[equal] and decrement greater by one.

#+BEGIN_SRC
f(A, p)
    smaller <-- 1
    equal <-- 1
    larger <-- A.length
    pivot <-- A[p]

    while equal < larger do
        if A[equal] < p do
            A[smaller] <-- A[equal]
            equal <-- equal + 1
            smaller <-- smaller + 1
        else if A[equal] = pivot do
            equal <-- equal + 1
        else
            A[larger] <-- A[equal]
            equal <-- equal - 1
#+END_SRC
*** Three Sum Closest
Given an array nums of nintegers and an integer target, find three integers in nums such that the num is closest to target. Return the sum of the three integers. You may assume that each input would have exactly one solution.

We can take inspiration from the two sum solution. In the two sum solution, if the array is sorted, we can use a two pointer solution where we adjust pointers accordingly depending on if the sum at both pointers is greater than or less than the target value. We can use take this solution and modify it for the purpose of threesum. Essentially we perform the two sum solution from another pointer i.

The slight catch here is that we are looking for the three sum that is closest to the target. So assuming that the target sum in the array does not exist, and we wish to find the closest sum we must take the absolute difference of the target and the current sum we are on. If the absolute difference is less than the current minimum we update the minimum value accordingly.

#+BEGIN_SRC
f(A, k)
    Sort(A)
    currentSum <-- A[1] + A[2] + A[A.length]
    currentMin <-- AbsDiff(currentSum - k)
    for i <-- 1 to A.length - 2 do
        p1 <-- i + 1
        p2 <-- A.length
        while p1 < p2 do
            sum <-- A[i] + A[p1] + A[p2]
            if sum > k do
                p2 <-- p2 - 1
            else
                p1 <-- p1 + 1
        if AbsDiff(sum - k) < currentMin do
            currentMin <-- AbsDiff(sum - k)
    return currentMin

#+END_SRC
